{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot  \n",
    "\n",
    "\n",
    "class VoiceSamples(Dataset):\n",
    "    \n",
    "    def __init__(self, core_name, samples_path=None, Automatic=None):\n",
    "        \n",
    "        self.Log = logging.getLogger()\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        \n",
    "        self.noiseThreshold = 1\n",
    "        \n",
    "        self.core_name = core_name\n",
    "        self.samples_path = samples_path\n",
    "        \n",
    "        self.soundSamples = []\n",
    "        self.sampleRate = []\n",
    "        self.path = []\n",
    "        \n",
    "        self.chopedSamples = []\n",
    "        self.chopedSr = []\n",
    "        \n",
    "        self.tensorMelgrams = []\n",
    "        \n",
    "        \n",
    "        self.info = \" VoiceSamples Object successfully created \"\n",
    "        self.Log.info(self.info)\n",
    "        \n",
    "        \n",
    "        if Automatic:\n",
    "            self.LoadSoundSamples()\n",
    "            self.ChopToOneSecFragments()\n",
    "            self.ChopedSignalsToTenosor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tensorMelgrams)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.tensorMelgrams:\n",
    "            return self.tensorMelgrams[idx]\n",
    "\n",
    "    def LoadSoundSamples(self):\n",
    "    \n",
    "        n = 1\n",
    "\n",
    "        while(True):\n",
    "            try:\n",
    "                if  self.samples_path:\n",
    "                    path =  self.samples_path + self.core_name + str(n)\n",
    "                else:\n",
    "                    path = self.core_name + str(n)\n",
    "\n",
    "                soundSample, sampleRate = lr.load(path)\n",
    "\n",
    "                n += 1\n",
    "                self.soundSamples.append(soundSample)\n",
    "                self.sampleRate.append(sampleRate) \n",
    "                self.path.append(path)\n",
    "\n",
    "                self.info = \" Sample : \" + path + \" : successfully added\"\n",
    "                self.Log.info(self.info)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                if self.soundSamples:\n",
    "                    self.info = \"That's the end of database : \" + str(n-1) + \" : Samples added\"\n",
    "                    self.Log.info(self.info)\n",
    "                    n = 0\n",
    "                    \n",
    "                    return self.soundSamples, self.sampleRate, self.path\n",
    "\n",
    "                else:\n",
    "                    self.Log.exception(\"Files are missing\")\n",
    "                    n = 0\n",
    "\n",
    "                break\n",
    "\n",
    "            except Exception as ex:      \n",
    "                self.Log.exception(\"Unexpected error\")\n",
    "                break\n",
    "        \n",
    "    def getSoundSample(self, idx):\n",
    "        return self.soundSamples[idx], self.sampleRate[idx]\n",
    "    \n",
    "    def getSoundSampleLen(self):\n",
    "        try:\n",
    "            if len(self.soundSamples) == len(self.sampleRate):\n",
    "                return len(self.soundSamples)\n",
    "            else:\n",
    "                self.Log.warning(\"Lists: sundSamples and sampleRate are not equal!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.Log.exception(\"Unexpected error\" + e)\n",
    "    \n",
    "    def ChopToOneSecFragments(self):\n",
    "        \n",
    "        # TODO: make shure user goes step by step \n",
    "        \n",
    "        try:\n",
    "            if len(self.soundSamples) == len(self.sampleRate):\n",
    "                for idx in range(len(self.soundSamples)):\n",
    "                    \n",
    "                    soundSample = self.soundSamples[idx]\n",
    "                    sr = self.sampleRate[idx]\n",
    "                    \n",
    "                    frag_max = math.trunc(len(soundSample)/float(sr))\n",
    "                    step = math.trunc(sr/2);\n",
    "                    last_sample = len(soundSample)\n",
    "\n",
    "                    for frag in range(frag_max*2):\n",
    "                        start = step * frag\n",
    "                        stop = start + sr\n",
    "                        if sr<len(soundSample):\n",
    "                            if self.checkIfNotNoise(soundSample[start:stop]):\n",
    "                                self.chopedSamples.append(soundSample[start:stop])\n",
    "                                self.chopedSr.append(sr)\n",
    "                                self.info = self.path[idx] + \" : \" + str(frag+1) + \" : successfully choped\"\n",
    "                                self.Log.info(self.info)\n",
    "                            else:\n",
    "                                self.info = self.path[idx] + \" : \" + str(frag+1) + \" : NOISE!\"\n",
    "                                self.Log.info(self.info)\n",
    "                        else:\n",
    "                            self.Log.warning(\"Something went wrong\")\n",
    "                            \n",
    "                    if self.checkIfNotNoise(soundSample[last_sample-sr:last_sample]):\n",
    "                         # incuding samples cuted by math.trunc() \n",
    "                        self.chopedSamples.append(soundSample[last_sample-sr:last_sample])\n",
    "                        self.chopedSr.append(sr)\n",
    "                        self.info = self.path[idx] +  \" : \"  + str(frag_max*2+1) + \" : successfully choped\"\n",
    "                        self.Log.info(self.info)\n",
    "                    else:\n",
    "                        self.info = self.path[idx] + \" : \"  + str(frag+1) + \" : NOISE!\"\n",
    "                        self.Log.info(self.info)\n",
    "                \n",
    "                if self.chopedSamples:\n",
    "                    self.Log.info(\"Sucessfully choped all loaded signals and eliminated the noise!\")\n",
    "                    return self.chopedSamples, self.chopedSr \n",
    "                    \n",
    "            else:\n",
    "                self.Log.warning(\"Lists: sundSamples and sampleRate are not equal!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.e = \"Unexpected error : \" + str(e)\n",
    "            self.Log.exception(self.e)\n",
    "            \n",
    "    def getChoped(self, idx):\n",
    "        return self.chopedSamples[idx], self.chopedSr[idx]\n",
    "        \n",
    "    def getChopedLen(self):\n",
    "        try:\n",
    "            if len(self.chopedSamples) == len(self.chopedSr):\n",
    "                    return len(self.chopedSamples)\n",
    "            else:\n",
    "                self.Log.warning(\"Lists: sundSamples and sampleRate are not equal!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.Log.exception(\"Unexpected error\" + e)\n",
    "            \n",
    "        \n",
    "    def ChopedSignalsToTenosor(self):\n",
    "        \n",
    "        # TODO: make shure user goes step by step \n",
    "        \n",
    "        try:\n",
    "        \n",
    "            if len(self.chopedSamples) == len(self.chopedSr):\n",
    "                for idx in range(len(self.chopedSamples)):\n",
    "\n",
    "                    # hop length adjusted\n",
    "                    STFT_signal = np.abs(lr.stft(self.chopedSamples[idx], n_fft = 512, hop_length = round(self.chopedSr[idx]/256))) \n",
    "                    STFT_signal = lr.power_to_db(STFT_signal**2,ref=np.max)\n",
    "\n",
    "                    Melgram = STFT_signal[0:256,0:256]\n",
    "                    TMelgram = torch.tensor(Melgram)\n",
    "                    self.tensorMelgrams.append(TMelgram)\n",
    "                    \n",
    "                    self.info = \" \" + self.samples_path +  \" : ChopedSample \" + str(idx) + \" : \" + \" : converted to tensor\"\n",
    "                    self.Log.info(self.info)\n",
    "                \n",
    "                if self.tensorMelgrams:\n",
    "                    self.Log.info(\"Sucessfully converted all ChopedSamples to Tensors!\")\n",
    "                    return self.tensorMelgrams\n",
    "                \n",
    "            else:\n",
    "                self.Log.warning(\"Lists: chopedSamples and chopedSr are not equal!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.e = \"Unexpected error : \" + str(e)\n",
    "            self.Log.exception(self.e)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def checkIfNotNoise(self, chopedSample):\n",
    "    \n",
    "        chopedSamplePow2 = []\n",
    "\n",
    "        for n in range(len(chopedSample)):\n",
    "            chopedSamplePow2.append(chopedSample[n]**2)\n",
    "        sk = sum(chopedSamplePow2)\n",
    "        if sk > self.noiseThreshold:\n",
    "            return True \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceRecogModel(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=61504, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VoiceRecogModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VoiceRecogModel, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*62*62, 1000)  # ?? from image dimension\n",
    "        self.fc2 = nn.Linear(1000, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    # create parrter recognition model \n",
    "\n",
    "PATH = \"C:/Users/AXDition/Documents/GitHub/NN_VoiceRecognition_Models/modelSave/modelV2.pt\"\n",
    "\n",
    "net = VoiceRecogModel()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vsKrystianTest1', 'vsNiciaTest1', 'vsNiciaTest2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"database/Test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vsNiciaTest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: VoiceSamples Object successfully created \n",
      "INFO:root: Sample : database/Test/vsNiciaTest1 : successfully added\n",
      "INFO:root: Sample : database/Test/vsNiciaTest2 : successfully added\n",
      "INFO:root:That's the end of database : 2 : Samples added\n",
      "INFO:root:database/Test/vsNiciaTest1 : 1 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 2 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 3 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 4 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 5 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 6 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest1 : 7 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 1 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 2 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 3 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 4 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 5 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 6 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 7 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 8 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 9 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 10 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 11 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 12 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 13 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 14 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 15 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 16 : successfully choped\n",
      "INFO:root:database/Test/vsNiciaTest2 : 17 : successfully choped\n",
      "INFO:root:Sucessfully choped all loaded signals and eliminated the noise!\n",
      "INFO:root: database/Test/ : ChopedSample 0 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 1 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 2 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 3 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 4 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 5 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 6 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 7 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 8 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 9 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 10 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 11 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 12 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 13 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 14 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 15 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 16 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 17 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 18 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 19 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 20 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 21 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 22 :  : converted to tensor\n",
      "INFO:root: database/Test/ : ChopedSample 23 :  : converted to tensor\n",
      "INFO:root:Sucessfully converted all ChopedSamples to Tensors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vsTest = VoiceSamples(input(), samples_path=\"database/Test/\" , Automatic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2099, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2099, 0.4426], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2760, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2760, 0.4483], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2145, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2145, 0.4285], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1087, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1087, 0.3964], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1987, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1987, 0.3963], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2304, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2304, 0.3940], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1772, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1772, 0.4255], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0144, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0144, 0.3893], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2232, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2232, 0.4398], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2721, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2721, 0.4373], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2564, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2564, 0.4693], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(0.9969, grad_fn=<MaxBackward0>) 0\n",
      "tensor([0.9969, 0.3604], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1179, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1179, 0.4118], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0672, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0672, 0.3808], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1328, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1328, 0.4502], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2406, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2406, 0.4250], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.2341, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.2341, 0.4933], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.1113, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.1113, 0.4171], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0779, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0779, 0.4006], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(0.9909, grad_fn=<MaxBackward0>) 0\n",
      "tensor([0.9909, 0.3600], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0275, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0275, 0.3820], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0743, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0743, 0.3597], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(1.0115, grad_fn=<MaxBackward0>) 0\n",
      "tensor([1.0115, 0.3774], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "tensor(0.9277, grad_fn=<MaxBackward0>) 0\n",
      "tensor([0.9277, 0.3615], grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "\n",
      "T: 24  F: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = 0\n",
    "f = 0\n",
    "# Training Loop\n",
    "\n",
    "for k in range(len(vsTest)):  \n",
    "    \n",
    "    vs = vsTest[k]\n",
    "    \n",
    "    input = vs.view(-1,1,256,256)\n",
    "    output = net(input)\n",
    "    v,i = output[0].max(0)\n",
    "    print(v,int(i))\n",
    "    if int(i) == 0:\n",
    "        t += 1\n",
    "    else:\n",
    "        f += 1\n",
    "    print(output[0])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"T:\",t,\" F:\",f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
